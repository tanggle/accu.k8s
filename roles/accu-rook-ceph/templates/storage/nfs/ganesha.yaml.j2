apiVersion: ceph.rook.io/v1
kind: CephNFS
metadata:
  name: {{ accu_ceph_ganesha_pool_name }}
  namespace: rook-ceph # namespace:cluster
spec:
  rados:
    # RADOS pool where NFS client recovery data is stored.
    # In this example the data pool for the "myfs" filesystem is used.
    # If using the object store example, the data pool would be "my-store.rgw.buckets.data".
    pool: {{ accu_ceph_ganesha_pool_name }}
    # RADOS namespace where NFS client recovery data is stored in the pool.
    namespace: {{ accu_ceph_ganesha_pool_namespace }}
  # Settings for the NFS server
  server:
    # the number of active NFS servers
    active: {{ accu_ceph_ganesha_instances }}
    # where to run the NFS server
    placement:
      nodeAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          nodeSelectorTerms:
          - matchExpressions:
            - key: node-role.kubernetes.io/ceph
              operator: In
              values:
              - storage
    #  topologySpreadConstraints:
{% if accu_rook_ceph_node_taint | bool %}
      tolerations:
      - key: accuinsight.io/ceph
        operator: Exists
        effect: NoSchedule
{% endif %}
    #  podAffinity:
    #  podAntiAffinity:
    # A key/value list of annotations
    annotations:
    #  key: value
    # The requests and limits set here allow the ganesha pod(s) to use half of one CPU core and 1 gigabyte of memory
    resources:
    #  limits:
    #    cpu: "500m"
    #    memory: "1024Mi"
    #  requests:
    #    cpu: "500m"
    #    memory: "1024Mi"
    # the priority class to set to influence the scheduler's pod preemption
    #priorityClassName:
    # The logging levels: NIV_NULL | NIV_FATAL | NIV_MAJ | NIV_CRIT | NIV_WARN | NIV_EVENT | NIV_INFO | NIV_DEBUG | NIV_MID_DEBUG |NIV_FULL_DEBUG |NB_LOG_LEVEL
    logLevel: NIV_INFO
