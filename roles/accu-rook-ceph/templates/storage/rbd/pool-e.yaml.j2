################################################################################
# Block Storage Pool for 'Erasure Coding'                                      #
################################################################################

apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: "{{ accu_ceph_block_pool_name }}.meta"
  namespace: rook-ceph
spec:
  # The failure domain will spread the replicas of the data across different failure zones
  failureDomain: {{ accu_ceph_block_storage_failuredomain }}
  # For a pool based on raw copies, specify the number of copies. A size of 1 indicates no redundancy.
  replicated:
    size: {{ accu_ceph_block_pool_replication_size }}
    # Disallow setting pool with replica 1, this could lead to data loss without recovery.
    # Make sure you're *ABSOLUTELY CERTAIN* that is what you want
    requireSafeReplicaSize: true
  # Ceph CRUSH root location of the rule
  # For reference: https://docs.ceph.com/docs/nautilus/rados/operations/crush-map/#types-and-buckets
  # crushRoot: my-root
  # The Ceph CRUSH device class associated with the CRUSH replicated rule
  # For reference: https://docs.ceph.com/docs/nautilus/rados/operations/crush-map/#device-classes
  #deviceClass: ssd 
  # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
  # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
  # enableRBDStats: true
  # Set any property on a given pool
  # see https://docs.ceph.com/docs/master/rados/operations/pools/#set-pool-values
  parameters:
    # Inline compression mode for the data pool
    # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression
    compression_mode: none
    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
    #target_size_ratio: ".5"
  # A key/value list of annotations
  annotations:
  #  key: value
---
apiVersion: ceph.rook.io/v1
kind: CephBlockPool
metadata:
  name: "{{ accu_ceph_block_pool_name }}.data"
  namespace: rook-ceph
spec:
  # The failure domain will spread the replicas of the data across different failure zones
  # Make sure you have enough nodes and OSDs running bluestore to support the replica size or erasure code chunks.
  # For the below settings, you need at least 3 OSDs on different nodes (because the `failureDomain` is `host` by default).
  failureDomain: {{ accu_ceph_block_storage_failuredomain }}
  erasureCoded:
    dataChunks: 2
    codingChunks: 1
  # Ceph CRUSH root location of the rule
  # For reference: https://docs.ceph.com/docs/nautilus/rados/operations/crush-map/#types-and-buckets
  # crushRoot: my-root
  # The Ceph CRUSH device class associated with the CRUSH replicated rule
  # For reference: https://docs.ceph.com/docs/nautilus/rados/operations/crush-map/#device-classes
  #deviceClass: ssd
  # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
  # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
  # enableRBDStats: true
  # Set any property on a given pool
  # see https://docs.ceph.com/docs/master/rados/operations/pools/#set-pool-values
  parameters:
    # Inline compression mode for the data pool
    # Further reference: https://docs.ceph.com/docs/nautilus/rados/configuration/bluestore-config-ref/#inline-compression
    compression_mode: none
    # gives a hint (%) to Ceph in terms of expected consumption of the total cluster capacity of a given pool
    # for more info: https://docs.ceph.com/docs/master/rados/operations/placement-groups/#specifying-expected-pool-size
    #target_size_ratio: ".5"
  # A key/value list of annotations
  annotations:
  #  key: value
